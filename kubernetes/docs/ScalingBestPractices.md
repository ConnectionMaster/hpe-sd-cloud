
**Table of Contents**

  - [Introduction](#introduction)
  - [Manually scaling](#manually-scaling)
  - [Autoscaling Deployments](#autoscaling-deployments)
  - [Autoscaling based in custom or external metrics](#autoscaling-based-in-custom-or-external-metrics)
  - [Vertical pod autoscaling](#vertical-pod-autoscaling)
  - [Best practices scaling Service Director pods](#best-practices-scaling-service-director-pods)


## Introduction

In a Kubernetes cluster, Service Director scalability defines how many replicas of the pods you'd like to run, when you increase or decrease the number of replicas and how many resources they need and setup the pods with those resources.

Autoscaling is the process of dynamically allocating resources to match performance requirements. As the volume of work grows in the SD pods, they may need additional resources to maintain the desired performance levels. When demand goes down and the additional resources are no longer needed, they can be de-allocated.

There are two main ways that an application can scale:

- Horizontal scaling, it means adding or removing instances of a resource. Service Director continues running without interruption as new pods are provisioned. When the provisioning process is complete, SD is deployed with these additional resources. If demand drops, the additional resources can be shut down cleanly and deallocated.

- Vertical scaling, it is used to change the capacity of a resource. For example, you could move an SD pod to a larger memory size. Vertical scaling often requires making the pod temporarily unavailable while it is being redeployed. Therefore, it's less common to automate vertical scaling but it can provide insights about the optimal resources needed for SD in the cluster.

Metrics Server is part of Kubernetes installation and it is a cluster-wide agregator of resource usage data. It collects metrics like CPU or memory consumption for containers or nodes, if the metrics-server plugin is installed in your cluster, you will be able to see the CPU and memory values for your cluster nodes or any of the pods. These metrics are useful for internal cluster sizing and it is needed for autoscaling tasks.

Horizontal scaling is executed using Horizontal Pod Autoscaler (HPA) objects. Each HPA object periodically checks a given metric against the target thresholds you configure, and scales up or down your pods automatically. If you configure a HPA to scale based on multiple metrics, HPA evaluates each metric separately and uses the scaling algorithm to determine the new workload scale based on each one.


## Manually scaling

The "kubectl scale" command lets your instantaneously change the number of replicas you want to run for your pod.

To use "kubectl scale", you specify the new number of replicas by setting the --replicas flag. To scale sd-sp pod to four replicas, run the following command, substituting controller for deployment, statefulset, or another controller object type:

     kubectl scale statefulset sd-sp --replicas 4

If successful, this command's output should be similar to

       statefulset "sd-sp" scaled.


## Autoscaling Deployments

You can autoscale SD deployments based on CPU or memory utilization of SD pods using the command "kubectl autoscale":

    kubectl autoscale deployment sd-sp --min=1 --max=10 --cpu-percent=50

That command will create a Horizontal Pod Autoscaler that will create between 1 and 10 replicas of the sd-sp pods to maintain an average CPU utilization across all pods of 50% .
Therefore if your cluster experience high CPU loads generated by these pods, the HPA will scale down the desired number of replicas.

You can find more information about autoscaling with the SD deployment [here](./AutoScaling.md)

Another way to use autoscaling is using a template like this one:


```yaml
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: servicedirector-hpa
  namespace: servicedirector
spec:
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Statefulset
    name: sd-sp
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      targetAverageUtilization: 50
  - type: Resource
    resource:
      name: memory
      targetAverageValue: 100Mi

```

If a targetAverageUtilization is specified, the currentMetricValue (cpu or memory) is computed by taking the average of the given metric across all pods. Before deciding on the final values, in this the average CPU utilization across all pods is calculated and check is exceed on 50%.

If a targetAverageValue is specified, the target value of the average of the resource metric across all relevant pods, as a raw value (instead of as a percentage of the request) is taken. Therefore the average memory utilization across all pods is calculated and check is exceed on 100 megabytes.



## Autoscaling based in custom or external metrics

Sometimes you want Kuberntes to take scaling decisions based in some metrics provided by an external source or a pod running in your cluster. There are some third party tools that add extra metrics to the Horizontal Pod Autoscaler and they are able to gather detailed performance information using Kubernetes metadata (deployments, services, pod).

Here are the high-level steps to accomplish the setup:

- Confirm the metric server is running in the Kubernetes cluster.
- Deploy the External Metric server that implements custom and external metrics API.
- Confirm the Service Director application is deployed and running in Kuberntes.
- Configure and deploy a new HPA that scales the SD deployment based on the new metrics.

You can find more information about custom autoscaling [here](./AutoScaling.md)


## Vertical pod autoscaling

You can use the VerticalPodAutoscaler custom resource to analyze and adjust your containers' CPU requests and memory requests. You can also configure a VerticalPodAutoscaler to make recommendations for CPU and memory requests, or you can configure it to make automatic changes to your CPU and memory requests.

This is an extra feature and must be installed in your K8S cluster, first of all you have to clone the kubernetes/autoscaler GitHub repository.

    git clone https://github.com/kubernetes/autoscaler.git

Change to the vertical-pod-autoscaler folder.

    cd autoscaler/vertical-pod-autoscaler/

Deploy the Vertical Pod Autoscaler to your cluster with the following command.

    ./hack/vpa-up.sh


Verify that the Vertical Pod Autoscaler pods have been created successfully.

     kubectl get pods -n kube-system

The output will be like this:

    NAME                                       READY   STATUS    RESTARTS   AGE
    vpa-admission-controller-f565bd55c-xv6gx   1/1     Running   0          2m18s
    vpa-recommender-7b77784f68-vvcc8           1/1     Running   0          2m19s
    vpa-updater-6754d65d8d-gk4pb               1/1     Running   0          2m20s



Here is a manifest for a VerticalPodAutoscaler:

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: sd-sp-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Statefulset
    name:       sd-sp
  updatePolicy:
    updateMode: "Off"
```

You can deploy the file with the following command:

    kubectl create vpa.yaml -n=servicedirector

Where vpa.yaml is the name of the file created with the VerticalPodAutoscaler.

Now you can run the following command to obtain resource information about your pod, in this case the sd-sp pods:

    kubectl get vpa sd-sp-vpa --output yaml -n=servicedirector


The output shows recommendations for CPU and memory requests:

```yaml
  recommendation:
    containerRecommendations:
    - containerName: sd-sp
      lowerBound:
        cpu: 25m
        memory: 262144k
      target:
        cpu: 25m
        memory: 262144k
      uncappedTarget:
        cpu: 25m
        memory: 262144k
      upperBound:
        cpu: 3168m
        memory: "22531313494"

```

Now that you have the recommended CPU and memory requests, you might choose to modify manually your SD deployment files and add CPU and memory requests to your Deployment manifest, and update your Deployment. You can also use this information to feed some Horizontal Pod Autoscaler with some custom resource values.


If you want to undeploy the Vertical Pod Autoscaler feature in your cluster, you can remove it with the following command:

    ./hack/vpa-down.sh


## Best practices scaling Service Director pods

This best practices will use the concepts that are introduced throughout this user guide:


### Measure the pods resource consumption in your cluster

Use the techniques included in [Vertical pod autoscaling](./ScalingBestPractices.md#vertical-pod-autoscaling) to capture your key metrics as CPU utilization and memory usage.

### Compare resource consumption with desired thresholds in your cluster

Evaluate these metrics against predefined thresholds or schedules, and decide whether to scale. Sometimes some applications take up more resources than you expected, this could be caused by pod overuse during peak times and it causes the pod to go out of control and use 100% of the available CPU, affecting some other pods in the node or cluster.

### Scale accordingly

Use the techniques included in [Autoscaling Deployments](./ScalingBestPractices.md#autoscaling-deployments) to setup limits on a per-pod basis.

If you don't feel comfortable assigning individual quotas to your SD pods, you can set up ResourceQuotas and LimitRanges at the servicedirector namespace level.

### Keep measuring and tuning

Monitor the resource usage and tune the autoscaling strategy regularly to ensure that it works as expected.




