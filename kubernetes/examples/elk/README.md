# Service Director ELK ready K8s Deployment Scenario

To ensure Service Director is available, performant and secure at all times, you can rely on the different types of logs generated. This data enables monitoring of SD and the identification and resolution of issues.

Architecture has evolved into containers infrastructure deployed on the cloud, across clouds or in hybrid environments. The volume of logs generated by SD is constantly growing and constitutes a challenge in itself. The ELK Stack integrated with SD fulfills the needs in the log management and analytics for the platform.

The ELK Stack helps by providing us with a powerful platform that collects and processes data from multiple SD logs, stores that data in one centralized data store that can scale as data grows, and that provides a set of tools to analyze the data.

Elasticsearch is the main piece of the ELK stack, it is a modern search and analytics engine that stores data in a way that allows using SQL statements to interact with the data. In the context of data analysis, Elasticsearch is used together with the other components in the ELK Stack, Logstash and Kibana, and plays the role of data indexing and storage.

Kibana is the analysis and visualization tool of the ELK stack, it uses a browser ui that allows to search, inspect and visualize the data stored in Elasticsearch indices. You can review your data more closely or in different ways, using a wide variety of different charts and graphs.

![Kubernetes cluster](./docs/images/SD_elk1.png)

This guide installs SD + ELK demo using basic Kubernetes commands without needing to download or install Helm. This installation lets you quickly evaluate SD + ELK in a Kubernetes cluster on any platform.

Filebeat is a lightweight component for collecting and forwarding log data. Installed as a sidecar container on the SD pod, Filebeat monitors the SD log files or locations that you specify, collects log events, and forwards them to Logstash for indexing and transformation.

The [sd-filebeat.yaml](./sd-filebeat.yaml) file defines a standard Service Director deployment for Kubernetes cluster with a Filebeat container attached as sidecar to SD-SP and another one to SD-UI, it is setup to send log information to an ELK stack through Logstash. This deployment defines a Filebeat sidecar container which reads some log files through shared volumes.

![Kubernetes cluster](./docs/images/SD_elk2.png)

For more information about ELK, please consult [ELK](https://www.elastic.co/what-is/elk-stack) page.

**IMPORTANT**: This configuration profile is not suitable for performance evaluation. It is designed to showcase ELK functionality with low levels of tracing and access logging.

**NOTE**: A guidance in the amount of Memory and Disk for the ELK k8s installation together with the full [sd-ha-deployment](/kubernetes/examples/sd-ha-deployment) is that it requires 8GB RAM, 4 CPUs and minimum 50GB free Disk space on the assigned k8s Node. The amount of Memory of cause depends of other applications/pods running in same node. In case k8s master and worker-node are in same host, like Minikube, then minimum 8GB RAM is required.

As a prerequisites for this deployment a database is required.


## Prerequisites
### 1. Deploy database

**If you have already deployed a database, you can skip this step!**

For this example, we bring up an instance of the `postgres` image in a K8S Pod, which is basically a clean PostgreSQL 11 image with a `sa` user ready for Service Director installation.

**NOTE**: If you are not using the k8s [postgres-db](../postgres-db) deployment, then you need to modify the [sd-filebeat](./sd-filebeat.yaml) database related environments to point to the used database.

The following databases are available:

- Follow the deployment as described in [postgres-db](../postgres-db) directory.
- Follow the deployment as described in [enterprise-db](../enterprise-db) directory.
- Follow the deployment as described in [oracle-db](../oracle-db) directory.

**NOTE**: For production environments you should either use an external, non-containerized database or create an image of your own, maybe based on official Postgres' [docker-images](https://hub.docker.com/_/postgres), EDB Postgres' [docker-images](http://containers.enterprisedb.com) or the official Oracle's [docker-images](https://github.com/oracle/docker-images).

**IMPORTANT**: Before deploying Service Director a namespace with the name "servicedirector" must be created. In order to generate the namespace, run:

    kubectl create namespace servicedirector


### 2. Deploy CouchDB

HPE Service Director UI relies on CouchDB as its data persistence module, in order to deploy CouchDB we use a Helm Chart to easily bring up the services.

Follow the deployment as described in the [CouchDB](../couchdb) example before moving to the following part.


### 3. Deploy ELK stack
To deploy the ELK stack we use three K8s deployment files to bring up the ELK services.

In order to install the ELK for Service Director into k8s cluster you have to start with Elasticsearch deployment:

    kubectl create -f elasticsearch.yaml

```
statefulset.apps/elasticsearch-logging created
service/elasticsearch-service created
```

then you have to deploy Logstash:

    kubectl create -f logstash.yaml

```
configmap/logstash-config created
deployment.extensions/logging-logstash created
service/logstash-service created
```

finally you have to deploy kibana:

    kubectl create -f kibana.yaml

```
deployment.apps/kibana-logging created
service/kibana-service created
```

Validate when the deployed ELK stack application/pod is ready (READY 1/1)

    kubectl get pods --namespace servicedirector

```
NAME                                    READY       STATUS      RESTARTS        AGE
elasticsearch-logging-0                 1/1         Running     0               4m12s
kibana-logging-657f7c6fc7-8rnmq         1/1         Running     0               40s
logstash-logging-7dd4fc7c84-z5flw       1/1         Running     0               2m12s
```

When the application is ready, then the deployed services are exposed with the following:

    kubectl get services --namespace servicedirector

```
NAME                    TYPE            CLUSTER-IP          EXTERNAL-IP     PORT(S)         AGE
elasticsearch-service   NodePort        10.97.199.250       <none>          9200/TCP        7m43s
kibana-service          NodePort        10.107.40.94        <none>          5601/TCP        4m22s
logstash-service        NodePort        10.96.148.115       <none>          5044/TCP        5m54s
```

When the ELK pods are ready, the Kibana UI is exposed on the following URL:

    http://<cluster_ip>:30033/      (Kibana UI)

**NOTE**: The Kubernetes `cluster_ip` can be found using the `kubectl cluster-info`.

To delete the ELK stack, run

    kubectl delete -f elasticsearch.yaml -f kibana.yaml -f logstash.yaml

```
statefulset.apps "elasticsearch-logging" deleted
service "elasticsearch-service" deleted
deployment.apps "kibana-logging" deleted
service "kibana-service" deleted
configmap "logstash-config" deleted
deployment.extensions "logstash-logging" deleted
service "logstash-service" deleted
```


### 4. Deploy SD-Filebeat

The [sd-filebeat.yaml](./sd-filebeat.yaml) file contains the following containers:

- `sd-sp`: HPE SD - [sd-sp](/docker/images/sd-sp)
- `sd-ui`: HPE SD UI - [sd-ui](/docker/images/sd-ui)
- `filebeat`: Filebeat monitors the log files or locations that you specify, collects log events, and forwards them to Logstash - [filebeat](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-overview.html)

Filebeat data folder stores a registry of read status for all log files, so it doesn't send everything again to Logstash on a pod restart. This folder must be created in a persistent storage therefore created outside the container, on the Kubernetes nodes. By default the folder is:

    mkdir /var/lib/filebeat-data
    chmod -R 777 /var/lib/filebeat-data

Be sure the folder has write permissions for the container to write on it. The path can be found under "volumes" in sd-filebeat.yaml

In order to deploy the Service Director + Filebeat pod, run:

    kubectl create -f sd-filebeat.yaml

```
configmap/filebeat-config created
deployment.apps/sdsp-deployment created
service/sdsp-nodeport created
service/sdui-nodeport created
```

**IMPORTANT**: The [sd-filebeat.yaml](./sd-filebeat.yaml) file uses defines a docker registry examples (`hub.docker.hpecorp.net/cms-sd`) for the used sd-sp images. This shall be changed to point to the docker registry where the docker images are located. E.g.: (`- image: hub.docker.hpecorp.net/cms-sd/sd-sp`)


You can validate if the deployed sd-filebeat applications/pods are ready (READY 3/3)

    kubectl get pods --namespace servicedirector

```
NAME                                       READY   STATUS    RESTARTS   AGE
sd-sp-0                                    2/2     Running   0          12m
ui-deployment-58649b6d86-kqkr9             2/2     Running   0          12m
```

When the SD pod is ready, then the deployed SD containers (SD User Interfaces) are exposed on the following urls:

    http://<kubernetes_cluster_ip>:32516/login      (Service Director UI)

    http://<kubernetes_cluster_ip>:32515/activator/ (Service Director native UI)

**NOTE**: The Kubernetes `cluster_ip` can be found using the `kubectl cluster-info`.

In order to guarantee that services are started in the right order, and to avoid a lot of initial restarts of the applications, until the prerequisites are fullfilled, this deployment file makes use of [RedinessProbes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/) and [livenessProbes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/) to the applications to do health check.

You can delete the deployed sd-filebeat applications/pods with the following command:

    kubectl delete -f sd-filebeat.yaml

```
configmap/filebeat-config deleted
deployment.apps/sdsp-deployment deleted
service/sdsp-nodeport deleted
service/sdui-nodeport deleted
```

To delete the PostgreSQL deployment, please follow the delete procedures as described in the respective examples.


### 5. How to check SD-ELK stack is working and first steps

Filebeat container collects the following SD log information and send it to Logstash pod:

- `SD container`: WildFly log using the following path - /opt/HP/jboss/standalone/log/
- `SD container`: Service Activator losg using the following path - /var/opt/OV/ServiceActivator/log/
- `SD container`: SNMP adapter log using the following path - /opt/sd-asr/adapter/log/
- `SD UI container`: UOC log using the following path - /var/opt//uoc2/logs

Logstash container also collects the Redis messages and send them to Elasticsearch:

Those logs are collected and transformed in Logstash, then are sent to Elasticsearch and stored with the following index name:

```
wildfly-YYYY.MM.dd      (JBoss log)
sa_mwfm-YYYY.MM.dd      (Service Activator log)
sa_resmgr-YYYY.MM.dd    (Service Activator log)
uoc-YYYY.MM.dd          (Unified OSS Console log)
redis-input-YYYY.MM.dd  (Redis messages)
```

You can check if the SD logs indexes were created and stored in Elasticsearch using the Kibana web interface, open the following URL in your browser:

    http://<kubernetes_cluster_ip>:30033/

Then select "Elasticsearch - Index Management" under the "Management" menu. Some of the logs mentioned previously should appear.

![Kubernetes cluster](./docs/images/SD_elk3.png)

Select "Kibana - Index Patterns" under the "Management" menu, then click on "Create Index Pattern" and select one of the log indexes.

![Kubernetes cluster](./docs/images/SD_elk4.png)

Now your log data will be available under the "Discover" menu.

![Kubernetes cluster](./docs/images/SD_elk5.png)


### Using a Service Director license

See in SD-SP [Using a Service Director License](../../deployments/sd-sp#using-a-service-director-license).


### Troubleshooting common problems

- Kibana is not displaying any Elasticsearch indexes.

  Check if Elasticsearch pod is up and running in the same namespace of Kibana pod, also review Logstash pod logs to check if connected succesfully to Elasticsearch pod.

- Kibana is not showing some of the SD logs.

  In order to generate data in the logs some activity must be recorded in SD. Login to SD web console and perform any task.

- Logstash pod is restarting all the time.

  If Logstash pod cannot connect to Elasticsearch pod will restart after some seconds. Check that Elasticsearch Service and Pod are up and running in the same namespace as Logstash.
