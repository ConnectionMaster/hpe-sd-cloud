
# Default values for sd-cl.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# global:
#   # affects sd images and subcharts (redis, kafka) but does not affect couchdb
#   imageRegistry:
#   storageClass:
#   # changes registry for all sdimages
#   sdimages:
#     registry:
#   # changes only sdsp/sdcl tag
#   sdimage:
#     tag:
#   prometheus:
#     enabled:
#   efk:
#     enabled:
#   pullPolicy:

monitoringNamespace:

install_assurance: true

sdsnmp_adapter:
  enabled: false

# affects all sd images
sdimages:
  registry:
  tag: 4.0.0
  pullPolicy: Always

## String to partially override sd-cl.fullname template (will maintain the release name)
##
# nameOverride:

## String to fully override sd-cl.fullname template
##
# fullnameOverride:

serviceAccount:
  enabled: false
  create: false
# name:
# imagePullSecrets:
# - name: my-secret-key

## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
securityContext:
  enabled: false
  fsGroup: 1001
  runAsUser: 1001

sdimage:
  licenseEnabled: false
  sshEnabled: false
  tag:
  podLabels: {}
    # key1: value1
  serviceLabels: {}
  env:
    SDCONF_activator_db_hostname: postgres-nodeport
    SDCONF_activator_db_instance: sa
    SDCONF_activator_db_password_key: dbpassword
    SDCONF_activator_db_password_name: sdsecrets
    SDCONF_activator_db_port:
    SDCONF_activator_db_user: sa
    SDCONF_activator_db_vendor: PostgreSQL
    SDCONF_activator_rolling_upgrade: no
    SDCONF_install_om: no
    SDCONF_install_omtmfgw: no
##  environment variables to control parameters in ActivatorConfig
    SDCONF_activator_conf_activation_max_threads:
    SDCONF_activator_conf_activation_min_threads:
    SDCONF_activator_conf_jvm_max_memory:
    SDCONF_activator_conf_jvm_min_memory:
    SDCONF_activator_conf_pool_defaultdb_max:
    SDCONF_activator_conf_pool_defaultdb_min:
    SDCONF_activator_conf_pool_inventorydb_max:
    SDCONF_activator_conf_pool_inventorydb_min:
    SDCONF_activator_conf_pool_mwfmdb_max:
    SDCONF_activator_conf_pool_mwfmdb_min:
    SDCONF_activator_conf_pool_resmgrdb_max:
    SDCONF_activator_conf_pool_resmgrdb_min:
    SDCONF_activator_conf_pool_servicedb_max:
    SDCONF_activator_conf_pool_servicedb_min:
    SDCONF_activator_conf_pool_uidb_max:
    SDCONF_activator_conf_pool_uidb_min:
    SDCONF_activator_conf_file_log_pattern:
  ports:
    name: 8080tcp01
    containerPort: 8080
# - Values needed during startup
  readinessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 10
  livenessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 30
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
  env_configmap_name:
  cpulimit: 5
  cpurequested: 3
  memorylimit: "3000Mi"
  memoryrequested: "1000Mi"
  securityContext:
    runAsUser:
  metrics:
    enabled: false
    proxy_enabled: true

statefulset_sdsp:
  replicaCount: 1
  app: sd-sp
  name: sd-sp
  servicename: sd-sp
  image:
    name: sd-sp
    registry:
    tag:

statefulset_sdcl:
  replicaCount: 1
  replicaCount_asr_only: 0
  dedicated_asr_node: false
  app: sd-cl
  name: sd-cl
  name_asr_only: sd-cl-asr-only
  servicename: sd-cl
  servicename_asr_only: sd-cl-asr-only
  image:
    name: sd-sp
    registry:
    tag:
  env:
    SDCONF_asr_kafka_brokers: kafka-service:9092
    SDCONF_asr_zookeeper_nodes: zookeeper-service:2181

service_sdsp:
  name: sd-sp
  port: 8080
  protocol: TCP
  servicetype: NodePort
  targetPort: 8080
  labels: {}
    # key1: value1

service_sdcl:
  name: sd-cl
  port: 8080
  protocol: TCP
  servicetype: NodePort
  targetPort: 8080
  labels: {}
    # key1: value1

sdui_image:
  replicaCount: 1
  podLabels: {}
    # key1: value1
  serviceLabels: {}
  app: sd-ui
  name: sd-ui
  servicename: sd-ui
  image:
    name: sd-ui
    registry:
    tag:
  env:
    SDCONF_sdui_provision_password_key: provisionpassword
    SDCONF_sdui_provision_password_name: sdsecrets
    SDCONF_sdui_provision_protocol: http
    SDCONF_sdui_provision_tenant: UOC_SD
    SDCONF_sdui_provision_use_real_user: no
    SDCONF_sdui_provision_username: admin
    SDCONF_sdui_log_format_pattern:
    SDCONF_uoc_couchdb_admin_password_key: adminPassword
    SDCONF_uoc_couchdb_admin_password_name: sd-helm-couchdb
    SDCONF_uoc_couchdb_admin_username_key: adminUsername
    SDCONF_uoc_couchdb_admin_username_name: sd-helm-couchdb
    SDCONF_uoc_couchdb_host: sd-helm-couchdb
    SDCONF_install_omui: no
  ports:
    containerPort: 3000
    name: 3000tcp01
# - Values needed during startup
  readinessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 10
  livenessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 30
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
  env_configmap_name:
  cpulimit: 3
  cpurequested: 1
  loadbalancer: false
  memorylimit: "3000Mi"
  memoryrequested: "500Mi"
  securityContext:
    runAsUser:

service_sdui:
  name: sd-ui
  port: 3000
  protocol: TCP
  servicetype: NodePort
  targetPort: 3000
  labels: {}
    # key1: value1

deployment_sdsnmp:
  replicaCount: 1
  podLabels: {}
    # key1: value1
  serviceLabels: {}
  app: sd-snmp-adapter
  name: sd-snmp-adapter
  image:
    name: sd-cl-adapter-snmp
    registry:
    tag:
  env:
    SDCONF_asr_adapters_bootstrap_servers: kafka-service:9092
    SDCONF_asr_adapters_manager_port:
  ports:
    containerPort: 162
    name: 162udp01
  readinessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 10
  livenessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 30
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
  env_configmap_name:
  cpulimit: 2
  cpurequested: 0.5
  memorylimit: "2000Mi"
  memoryrequested: "500Mi"
  securityContext:
    runAsUser:

service_sdsnmp:
  name: sd-snmp-adapter
  port: 162
  protocol: UDP
  targetPort: 162
  servicetype: NodePort
  labels: {}
    # key1: value1

healthcheck:
  enabled: false
  tag: 1.0.0
  registry:
  name: sd-healthcheck
  labelfilter:
    unhealthy:
      - "app:sd-sp"
    degraded:
      - "app:sd-ui"
      - "app:couchdb"
      - "app:redis"
      - "app.kubernetes.io/name:kafka"
      - "app.kubernetes.io/name:zookeeper"
      - "app:sd-healthcheck"
  resources:
    limits:
      cpu: 400m
      memory: 500Mi
    requests:
      memory: 256Mi
      cpu: 250m
  securityContext:
    fsGroup: 1001
    runAsUser: 1001
  serviceaccount:
    enabled: false
    name: sd-healthcheck

kafka:
  enabled: false
  fullnameOverride: "kafka-service"
  image:
    pullPolicy: Always
  persistence:
    enabled: false
  resources:
    limits:
      cpu: 400m
      memory: 1Gi
    requests:
      cpu: 250m
  securityContext:
    enabled: false
    fsGroup: 1001
    runAsUser: 1001
  metrics:
    kafka:
      enabled: false
    jmx:
      enabled: false
  zookeeper:
    fullnameOverride: "zookeeper-service"
    image:
      pullPolicy: Always
    metrics:
      enabled: false
    persistence:
      enabled: false
    securityContext:
      enabled: false
      fsGroup: 1001
      runAsUser: 1001
    resources:
      limits:
        cpu: 400m
        memory: 512Mi
      requests:
        cpu: 250m

couchdb:
  enabled: true
  createAdminSecret: false
  fullnameOverride: "uoc"
  clusterSize: 1
  persistentVolume:
    enabled: false
  couchdbConfig:
    couchdb:
      uuid: decafbaddecafbaddecafbaddecafbad
  image:
    pullPolicy: Always
  initImage:
    pullPolicy: Always

redis:
  enabled: true
  fullnameOverride: "redis"
  redisPort: 6379
  existingSecret: redis-password
  existingSecretPasswordKey: password
  image:
    pullPolicy: Always
  metrics:
    enabled: false
  master:
    persistence:
      enabled: false
  slave:
    persistence:
      enabled: false
  cluster:
    enabled: false
    slaveCount: 0
  securityContext:
    enabled: false
    fsGroup: 1001
    runAsUser: 1001

prometheus:
  ## Declare this to enable/disable local prometheus
  # enabled: false
  image:
    registry:
    name: prom/prometheus
    tag: v2.30.3
  server_enabled: true
  alertmanager:
    enabled: false
    image:
      registry:
      name: prom/alertmanager
      tag: v0.23.0
  podLabels: {}
    # key1: value1
  serviceLabels: {}
  pullPolicy: Always
  servicetype: NodePort
  grafanaservicetype: NodePort
  memoryrequested: "500Mi"
  cpurequested: "200m"
  memorylimit:
  cpulimit:
  grafana:
    enabled: true
    image:
      registry:
      name: grafana/grafana
      tag: 8.2.1
    memoryrequested: "100Mi"
    cpurequested: "200m"
    memorylimit:
    cpulimit:
  ksm:
    image:
      registry: quay.io/
      name: coreos/kube-state-metrics
      tag: v1.9.8
    memoryrequested: "50Mi"
    cpurequested: "100m"
    memorylimit:
    cpulimit:

service_sdsp_prometheus:
  labels: {}
    #key1: value

service_sdcl_prometheus:
  labels: {}
    #key1: value

service_sd_ksm:
  labels: {}
    #key1: value
    
service_grafana:
  labels: {}
    #key1: value

efk:
  ## Declare this to enable/disable local efk
  # enabled: false
  image:
    registry: docker.elastic.co/
    name: elasticsearch/elasticsearch
    tag: 7.10.1
  pullPolicy: Always
  podLabels: {}
    # key1: value1
  elastalert:
    enabled: false
    image:
      registry:
      name: bitsensor/elastalert
      tag: 2.0.1
    efkserver: "elasticsearch-service:9200"
  elastic:
    enabled: true
    replicas: 1
    extraVolumes:
    # - name: backup-dir
    # mountPath: /mnt/nfs/backup/repo
    extraVolumeMounts:
    # - name: backup-dir
    # emptyDir: {}
    extraInitContainers:
    # - name: data-permissions
    # image: busybox
    # imagePullPolicy: IfNotPresent
    # command: ['sh', '-c', 'chown 1000 /data']
    # volumeMounts:
    # - name: elasticsearch-data
    #   mountPath: /data
    persistence: false
    memoryrequested: "1.3Gi"
    cpurequested: "500m"
    servicetype: NodePort
    memorylimit: "2Gi"
    cpulimit: "1000m"
    runAsUser:
    # storageClass: "sdstorageclass"
  kibana:
    image:
      registry: docker.elastic.co/
      name: kibana/kibana
      tag:
    enabled: true
    servicetype: NodePort
    memoryrequested: "400Mi"
    cpurequested: "300m"
    memorylimit: "400Mi"
    cpulimit: "1000m"
  fluentd:
    enabled: true
    elasticserver:
    elasticport: "9200"

service_efk:
  labels: {}
    #key1: value

fluentd:
  image:
    registry:
    name: bitnami/fluentd
    tag: 1.14.1
  memoryrequested: "512Mi"
  cpurequested: "300m"
  memorylimit: "1Gi"
  cpulimit: "500m"

envoy:
  image:
    registry:
    name: bitnami/envoy
    tag: 1.16.5

ingress:
  enabled: false
  annotations:
  hosts:
  - name:
    sdenabled: true
    sduienabled: true
