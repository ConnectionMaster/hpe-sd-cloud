
# Default values for sd-cl.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# global:
#   # affects sd images and subcharts (redis, kafka) but does not affect couchdb
#   imageRegistry:
#   storageClass:
#   # changes registry for all sdimages
#   sdimages:
#     registry:
#   # changes only sdsp/sdcl tag
#   sdimage:
#     tag:
#   prometheus:
#     enabled:
#   efk:
#     enabled:
#   pullPolicy:

monitoringNamespace:

install_assurance: true

secrets_as_volumes: true

enable_rolling_upgrade: false

sdsnmp_adapter:
  enabled: false

muse:
  enabled: true

sd_ui_uoc:
  enabled: false

# affects all sd images
sdimages:
  registry:
  tag: 4.2.7
  pullPolicy: Always

## String to partially override sd-cl.fullname template (will maintain the release name)
##
# nameOverride:

## String to fully override sd-cl.fullname template
##
# fullnameOverride:

serviceAccount:
  enabled: false
  create: false
# name:
# imagePullSecrets:
# - name: my-secret-key

automountServiceAccountToken:
  enabled: false

## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
securityContext:
  enabled: false
  fsGroup: 1001
  runAsUser: 1001
  readOnlyRootFilesystem: false
  enableRunAsUser: true
  dropAllCapabilities: false
  # once all capabilities are dropped, add allowed here
  # addCapabilities:
  #   - NET_BIND_SERVICE

sdimage:
  licenseEnabled: false
  sshEnabled: false
  tag:
  podLabels: {}
    # key1: value1
  serviceLabels: {}
  env:
    SDCONF_activator_db_hostname: postgres-nodeport
    SDCONF_activator_db_instance: sa
    # Password key to be retrieved from DB Secret, if empty it will retrieve a key set by default automatically
    SDCONF_activator_db_password_key:
    # Name of DB Secret, if left empty, a default secret will be created automatically
    # For more information, refer to the "Managing Secrets" section of the README
    SDCONF_activator_db_password_name:
    SDCONF_activator_db_port:
    SDCONF_activator_db_user: sa
    SDCONF_activator_db_vendor: PostgreSQL
    SDCONF_activator_rolling_upgrade: no
    SDCONF_install_om: no
    SDCONF_install_omtmfgw: no
##  environment variables to control parameters in ActivatorConfig
    SDCONF_activator_conf_activation_max_threads:
    SDCONF_activator_conf_activation_min_threads:
    SDCONF_activator_conf_jvm_max_memory:
    SDCONF_activator_conf_jvm_min_memory:
    SDCONF_activator_conf_pool_defaultdb_max:
    SDCONF_activator_conf_pool_defaultdb_min:
    SDCONF_activator_conf_pool_inventorydb_max:
    SDCONF_activator_conf_pool_inventorydb_min:
    SDCONF_activator_conf_pool_mwfmdb_max:
    SDCONF_activator_conf_pool_mwfmdb_min:
    SDCONF_activator_conf_pool_resmgrdb_max:
    SDCONF_activator_conf_pool_resmgrdb_min:
    SDCONF_activator_conf_pool_servicedb_max:
    SDCONF_activator_conf_pool_servicedb_min:
    SDCONF_activator_conf_pool_uidb_max:
    SDCONF_activator_conf_pool_uidb_min:
    SDCONF_activator_conf_file_log_pattern:
    SDCONF_activator_conf_jboss_log_max_days:
    SDCONF_activator_conf_resmgr_log_max_files:
    SDCONF_activator_conf_wfm_log_max_files:
  ports:
    name: 8080tcp01
    containerPort: 8080
# - Values needed during startup
  readinessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 10
  livenessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 30
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
  env_configmap_name:
  cpulimit: 5
  cpurequested: 1
  memorylimit: "3000Mi"
  memoryrequested: "1000Mi"
  securityContext:
    runAsUser:
  metrics:
    enabled: false
    proxy_enabled: true
  # emptydirs:
  #   docker: /docker
  #   asr: /etc/opt/OV/ServiceActivator/ASR
  #   config: /etc/opt/OV/ServiceActivator/config
  #   designer: /etc/opt/OV/ServiceActivator/designer
  #   newconfig: /etc/opt/OV/ServiceActivator/newconfig
  #   sql: /etc/opt/OV/ServiceActivator/sql
  #   template-files: /etc/opt/OV/ServiceActivator/template_files
  #   jboss-bin: /opt/HP/jboss/bin
  #   jboss-standalone: /opt/HP/jboss/standalone
  #   sa-bin: /opt/OV/ServiceActivator/bin
  #   solutions: /opt/OV/ServiceActivator/solutions
  #   sa: /var/opt/OV/ServiceActivator

statefulset_sdsp:
  replicaCount: 1
  app: sd-sp
  name: sd-sp
  servicename: sd-sp
  image:
    name: sd-sp
    registry:
    tag:

statefulset_sdcl:
  replicaCount: 1
  replicaCount_asr_only: 0
  dedicated_asr_node: false
  app: sd-cl
  name: sd-cl
  name_asr_only: sd-cl-asr-only
  servicename: sd-cl
  servicename_asr_only: sd-cl-asr-only
  image:
    name: sd-sp
    registry:
    tag:
  env:
    SDCONF_asr_kafka_brokers: kafka-service:9092
    SDCONF_asr_zookeeper_nodes: zookeeper-service:2181

service_sdsp:
  name: sd-sp
  port: 8080
  protocol: TCP
  servicetype: NodePort
  targetPort: 8080
  labels: {}
    # key1: value1
  extraPorts:
    # - name: another-port
    #   port: 1234
    #   targetPort: 1234

service_sdcl:
  name: sd-cl
  port: 8080
  protocol: TCP
  servicetype: NodePort
  targetPort: 8080
  labels: {}
    # key1: value1

muse_container:
  master_node_ip:
  servicePrefix: sv-
  replicaCount: 1
  tag: 1.2.0
  pullPolicy: IfNotPresent
  port: 8080
  serviceType: NodePort
  sessionAffinity: None
  env:
    DB_TYPE: postgres
    DB_HOST: postgres-nodeport
    DB_PORT: 5432
    DB_NAME: muse
    DB_USER: sa
    # DB password for MUSE services to connect to the database
    DB_SECRET_NAME:
    DB_SECRET_KEY:
    DB_RETRY_TRIES:
    DB_RETRY_TIMEOUT:
    DB_SSL_ENABLED: "n"
    DB_SSL_STRICT: "y"
    DB_SSL_CERTIFICATE: # Full path to the file where the certificate is stored.
    DB_SSL_SECURE_PROTOCOL: TLSv1_2_method # Indicates the secure protocol to be used.
    REDIS_HOST:
    REDIS_PORT:
    REDIS_TLS_ENABLED:
    REDIS_TLS_STRICT_SSL:
    REDIS_TLS_CERTIFICATE:
    REDIS_TLS_PRIVATE_KEY:
    REDIS_TLS_SECURE_PROTOCOL:
    REDIS_TLS_ROOT_CERTIFICATES:
    MUSE_JWT_SECRET:
    SERVICES_PROTOCOL: http
  startupProbe:
    periodSeconds: 10
    timeoutSeconds: 10
    failureThreshold: 15
  livenessProbe:
    periodSeconds: 10
    timeoutSeconds: 10
    failureThreshold: 15
  readinessProbe:
    periodSeconds: 10
    timeoutSeconds: 10
    failureThreshold: 18
  resources:
    enabled: true
    requests:
      memory: "150Mi"
      cpu: 0.5
    limits:
      memory: "500Mi"
      cpu: 0.5

muse_shell:
  enabled: true
  replicaCount:
  name: muse-shell
  podLabels: {}
  port: 8080
  image:
    name: muse-shell
    registry:
    tag:
  env:
    # these parameters must we filled with your own server and port configurations
    # Usually are the host name where the Ingress is installed and the nodeport exposed by the Ingress
    AUTH_ENDPOINT: "http://kubernetes_exposed_host:kubernetes_exposed_port/muse-auth"
    REGISTRY_ENDPOINT: "http://kubernetes_exposed_host:kubernetes_exposed_port/muse-registry"
    NOTIFICATION_ENDPOINT: "http://kubernetes_exposed_host:kubernetes_exposed_port/muse-notif"
    CONFIGURATION_ENDPOINT: "http://kubernetes_exposed_host:kubernetes_exposed_port/muse-conf"
  serviceLabels: {}
  serviceType:
  nodePort:
  sessionAffinity: ClientIP
  # startupProbe:
  #   periodSeconds: 10
  #   timeoutSeconds: 10
  #   failureThreshold: 18
  # livenessProbe:
  #   periodSeconds: 10
  #   timeoutSeconds: 10
  #   failureThreshold: 18
  # readinessProbe:
  #   periodSeconds: 10
  #   timeoutSeconds: 10
  #   failureThreshold: 21
  # resources:
  #   requests:
  #     memory: "350Mi"
  #     cpu: 0.5
  #   limits:
  #     memory: "500Mi"
  #     cpu: 0.5

muse_auth:
  enabled: true
  replicaCount:
  name: muse-auth
  podLabels: {}
  port: 4000
  image:
    name: muse-auth-service
    registry:
    tag:
  env:
    LOG_LEVEL: info
# JWT Parameters. Specify if you want to override defaults
    JWT_SECRET_REFRESH:
    JWT_TIMEOUT_ACCESS:
    JWT_TIMEOUT_REFRESH:
    SHELL_LOGIN_CALLBACK: http://localhost/auth/callback
    SHELL_LOGOUT_CALLBACK: http://localhost
    AUTH_TYPE: local
    OIDC_LOGIN_CALLBACK:
    OIDC_LOGOUT_CALLBACK:
    OIDC_ISSUER:
  serviceLabels: {}
  serviceType:
  nodePort:

muse_registry:
  enabled: true
  replicaCount:
  name: muse-regitry-discover
  podLabels: {}
  port: 4001
  image:
    name: muse-registry-discover-service
    registry:
    tag:
  env:
    NOTIFICATION_ENABLED: false
  serviceLabels: {}
  serviceType:
  nodePort:

muse_notif:
  enabled: true
  replicaCount:
  name: muse-notification-service
  podLabels: {}
  port: 4002
  image:
    name: muse-notification-service
    registry:
    tag:
  serviceLabels: {}
  serviceType:
  nodePort:

muse_configuration:
  enabled: true
  replicaCount:
  name: muse-configuration
  podLabels: {}
  port: 4003
  image:
    name: muse-configuration-service
    registry:
    tag:
  serviceLabels: {}
  serviceType:
  nodePort:

muse_sd_ui:
  enabled: true
  replicaCount:
  name: muse-sd-ui
  podLabels: {}
  port: 8080
  image:
    name: muse-sd-ui
    registry:
    tag: 4.2.7
  env:
    USER_NAME: admin
    # Password that allows configuration in HPE MUSE
    PASSWORD_SECRET_NAME:
    PASSWORD_SECRET_KEY:
    CONFIGURE_USERS: "y"
    SET_VIEWS_VISIBLE: "y"
     # these parameters must we filled with your own server and port configurations
     # Usually are the host name where the Ingress is installed and the nodeport exposed by the Ingress
    UI_PLUGIN_HOST: "kubernetes_exposed_host"
    UI_PLUGIN_PORT: "kubernetes_exposed_port"
    HPESD_PORT: 8080
     # these parameters must we filled with your own server and port configurations
     # Usually are the host name where the Ingress is installed and the nodeport exposed by the Ingress
    HPESD_EXPOSED_HOST: "kubernetes_exposed_host"
    HPESD_EXPOSED_PORT: "kubernetes_exposed_port"

muse_sd_ui_plugin:
  enabled: true
  replicaCount:
  name: muse-sd-ui-plugin
  podLabels: {}
  port: 3001
  image:
    name: sd-ui-plugin
    registry:
    tag: 4.2.7
  env:
    HPSA_TENANT: UOC_SD
    HPSA_USER: admin
    HPSA_PASSWORD_SECRET_NAME:
    HPSA_PASSWORD_SECRET_KEY:
    LOG_LEVEL: INFO
    ATTACHMENT_ENABLED: "n"
    ATTACHMENT_PASSWORD_SECRET_NAME:
    ATTACHMENT_PASSWORD_SECRET_KEY:

sdui_image:
  replicaCount: 1
  podLabels: {}
    # key1: value1
  serviceLabels: {}
  app: sd-ui
  name: sd-ui
  servicename: sd-ui
  image:
    name: sd-ui
    registry:
    tag:
  env:
    # Password key to be retrieved from Provisioner Secret, if empty it will retrieve a key set by default
    SDCONF_sdui_provision_password_key:
    # Name of Provisioner Secret, if left empty, a default secret will be created automatically
    # For more information, refer to the "Managing Secrets" section of the README
    SDCONF_sdui_provision_password_name:
    SDCONF_sdui_provision_protocol: http
    SDCONF_sdui_provision_tenant: UOC_SD
    SDCONF_sdui_provision_use_real_user: no
    SDCONF_sdui_provision_username: admin
    SDCONF_sdui_provision_idp: no
    SDCONF_sdui_provision_idp_reuse_token: no
    SDCONF_sdui_log_format_pattern:
    # Keys to be retrieved from CouchDB Secret
    SDCONF_uoc_couchdb_admin_password_key: adminPassword
    SDCONF_uoc_couchdb_admin_username_key: adminUsername
    SDCONF_uoc_couchdb_host: sd-helm-couchdb
    SDCONF_install_omui: no
    SDCONF_sdui_uoc_protocol: http
    SDCONF_sdui_idp: no
    SDCONF_sdui_idp_issuer:
    SDCONF_sdui_idp_entry_point:
    SDCONF_sdui_idp_identifier_format:
    SDCONF_sdui_idp_accepted_clock_skew_ms:
    SDCONF_sdui_oidc: no
    SDCONF_sdui_oidc_issuer:
    SDCONF_sdui_oidc_authorization_endpoint:
    SDCONF_sdui_oidc_token_endpoint:
    SDCONF_sdui_oidc_user_info_endpoint:
    SDCONF_sdui_oidc_jwks_uri:
    SDCONF_sdui_oidc_end_session_endpoint:
    SDCONF_sdui_oidc_check_session_endpoint:
    SDCONF_sdui_oidc_client_id:
    SDCONF_sdui_oidc_client_secret:
    SDCONF_sdui_oidc_redirect_uri:
    SDCONF_sdui_oidc_post_logout_redirect_uri:
    SDCONF_sdui_oidc_silent_redirect_uri:
    SDCONF_sdui_oidc_token_endpoint_auth_method:
    SDCONF_sdui_oidc_id_token_signed_response_alg:
    SDCONF_sdui_oidc_user_info_signed_response_alg:
    SDCONF_sdui_oidc_response_type:
    SDCONF_sdui_oidc_post_auth_callback:
    SDCONF_sdui_oidc_scope:
  uoc_certificate_secret:
  idp_certificate_secret:
  ports:
    containerPort: 3000
    name: 3000tcp01
# - Values needed during startup
  readinessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 10
  livenessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 30
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
  env_configmap_name:
  cpulimit: 500m
  cpurequested: 500m
  loadbalancer: false
  memorylimit: "500Mi"
  memoryrequested: "500Mi"
  securityContext:
    runAsUser:
  # emptydirs:
  #   docker: /docker
  #   bin: /opt/uoc2/bin
  #   client-addons: /opt/uoc2/client/addons
  #   users: /opt/uoc2/data/users
  #   addons-plugins: /opt/uoc2/server/addons/plugins
  #   public-addons-plugins: /opt/uoc2/server/public/addons/plugins
  #   conf: /opt/uoc2/server/public/conf
  #   permissions: /var/opt/uoc2/data/permissions

service_sdui:
  name: sd-ui
  port: 3000
  protocol: TCP
  servicetype: NodePort
  targetPort: 3000
  labels: {}
    # key1: value1

deployment_sdsnmp:
  replicaCount: 1
  podLabels: {}
    # key1: value1
  serviceLabels: {}
  app: sd-snmp-adapter
  name: sd-snmp-adapter
  image:
    name: sd-cl-adapter-snmp
    registry:
    tag:
  env:
    SDCONF_asr_adapters_bootstrap_servers: kafka-service:9092
    SDCONF_asr_adapters_manager_port:
  ports:
    containerPort: 162
    name: 162udp01
  readinessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 10
  livenessProbe:
    failureThreshold: 2
    periodSeconds: 5
    initialDelaySeconds: 30
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
  env_configmap_name:
  cpulimit: 2
  cpurequested: 0.5
  memorylimit: "2000Mi"
  memoryrequested: "500Mi"
  securityContext:
    runAsUser:
  # emptydirs:
  #   docker: /docker
  #   adapter: /opt/sd-asr/adapter/

service_sdsnmp:
  name: sd-snmp-adapter
  port: 162
  protocol: UDP
  targetPort: 162
  servicetype: NodePort
  labels: {}
    # key1: value1

healthcheck:
  app: sd-healthcheck
  enabled: false
  tag: 1.0.11
  registry:
  name: sd-healthcheck
  labelfilter:
    unhealthy:
      - "app:sd-cl"
    degraded:
      - "app:sd-ui"
      - "app:couchdb"
      - "app.kubernetes.io/name:redis"
      - "app.kubernetes.io/name:kafka"
      - "app.kubernetes.io/name:zookeeper"
      - "app:sd-healthcheck"
  templateOutput:
    enabled: false
  resources:
    limits:
      cpu: 400m
      memory: 500Mi
    requests:
      memory: 256Mi
      cpu: 250m
  readinessProbe:
    failureThreshold: 2
    periodSeconds: 5
  livenessProbe:
    initialDelaySeconds: 30
    failureThreshold: 2
    periodSeconds: 5
  startupProbe:
    failureThreshold: 10
    periodSeconds: 10
  securityContext:
    fsGroup: 1001
    runAsUser: 1001
  serviceaccount:
    enabled: false
    name: sd-healthcheck
  labels: {}
    # key1: value1
  podLabels: {}
    # key1: value1
  serviceLabels: {}
    # key1: value1
  env:
    log_level: INFO
    https_enabled: false    
  metrics:
    enabled: false

service_healthcheck:
  servicetype: ClusterIP

kafka:
  enabled: false
  fullnameOverride: "kafka-service"
  image:
    tag: 3.3.2-debian-11-r0
    pullPolicy: Always
  persistence:
    enabled: false
  resources:
    limits:
      cpu: 400m
      memory: 1Gi
    requests:
      cpu: 250m
      memory: 256Mi
  podSecurityContext:
    enabled: false
    fsGroup: 1001
  containerSecurityContext:
    enabled: false
    runAsUser: 1001
  metrics:
    kafka:
      enabled: false
    jmx:
      enabled: false
  startupProbe:
    enabled: true
  zookeeper:
    fullnameOverride: "zookeeper-service"
    image:
      tag: 3.8.0-debian-11-r82
      pullPolicy: Always
    metrics:
      enabled: false
    persistence:
      enabled: false
    podSecurityContext:
      enabled: false
      fsGroup: 1001
    containerSecurityContext:
      enabled: false
      runAsUser: 1001
    resources:
      limits:
        cpu: 400m
        memory: 512Mi
      requests:
        cpu: 250m

couchdb:
  ## If createAdminSecret is enabled a Secret called uoc-couchdb will
  ## be created containing auto-generated credentials.
  ## Set createAdminSecret to disabled in case you want to use you own secret.
  ## In this case you will have to create you own uoc-couchdb secret in advance of the deployment.
  createAdminSecret: true
  fullnameOverride: "uoc"
  clusterSize: 1
  persistentVolume:
    enabled: false
  couchdbConfig:
    couchdb:
      uuid: decafbaddecafbaddecafbaddecafbad
  erlangFlags:
    setcookie: secret
  image:
    tag: 3.3.1
    pullPolicy: Always
  initImage:
    pullPolicy: Always
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 400m
      memory: 1Gi
  readinessProbe:
    timeoutSeconds: 5

redis:
  enabled: true
  fullnameOverride: "redis"
  redisPort: 6379
  auth:
    # If this parameter is set, Redis will use this existing secret
    existingSecret:
    # Password key to be retrieved from existing secret
    existingSecretPasswordKey:
  image:
  #  tag: 7.0.8-debian-11-r0 
    pullPolicy: Always
  metrics:
    enabled: false
  master:
    persistence:
      enabled: false
    resources:
      limits:
        cpu: 100m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 256Mi
    podSecurityContext:
      enabled: false
      fsGroup: 1001
    containerSecurityContext:
      enabled: false
      runAsUser: 1001
  replica:
    replicaCount: 1
    persistence:
      enabled: false
    resources:
      limits:
        cpu: 100m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 256Mi
    podSecurityContext:
      enabled: false
      fsGroup: 1001
    containerSecurityContext:
      enabled: false
      runAsUser: 1001

prometheus:
  ## Declare this to enable/disable local prometheus
  # enabled: false
  image:
    registry:
    name: prom/prometheus
    tag: v2.41.0
  server_enabled: true
  alertmanager:
    enabled: false
    image:
      registry:
      name: prom/alertmanager
      tag: v0.24.0
  podLabels: {}
    # key1: value1
  serviceLabels: {}
  pullPolicy: Always
  servicename: prometheus-service
  serviceport: 8080
  servicetype: NodePort
  grafanaservicetype: NodePort
  memoryrequested: "500Mi"
  cpurequested: "200m"
  memorylimit: "500Mi"
  cpulimit: "200m"
  scrape_interval:
  evaluation_interval:
  scrape_timeout:
  extraContainers: []
  extraVolumes: []
  # - name: example-config-volume
  #   configMap:
  #     defaultMode: 420
  #     name: example-config
  extraVolumeMounts: []
  # - name: example-volume
  #   mountPath: /example/
  customJobs: []
  # - job_name: 'example-job'
  #   metrics_path: /test
  #   scheme: http
  #   tls_config:
  #     insecure_skip_verify: true
  #   static_configs:
  #     - targets: ["{{ .Values.example }}.{{ .Release.Namespace }}.svc.cluster.local:8080"]
  grafana:
    enabled: true
    image:
      registry:
      name: grafana/grafana
      tag: 8.5.16
    memoryrequested: "100Mi"
    cpurequested: "200m"
    memorylimit: "100Mi"
    cpulimit: "200m"
    extraDashboardsConfigmaps: []
    # - name: example-dashboard-configmap
    #   dashboardFile: Example-dashboard.json
  ksm:
    image:
      registry: quay.io/
      name: coreos/kube-state-metrics
      tag: v1.9.8
    memoryrequested: "50Mi"
    cpurequested: "100m"
    memorylimit: "50Mi"
    cpulimit: "100m"

service_sd_envoy:
  labels: {}
    #key1: value
  servicetype: NodePort

service_sd_ksm:
  labels: {}
    #key1: value

service_grafana:
  labels: {}
    #key1: value

efk:
  ## Declare this to enable/disable local efk
  # enabled: false
  image:
    registry: docker.elastic.co/
    name: elasticsearch/elasticsearch
    tag: 7.10.1
  pullPolicy: Always
  podLabels: {}
    # key1: value1
  elastalert:
    enabled: false
    image:
      registry:
      name: bitsensor/elastalert
      tag: 2.0.1
    efkserver: "elasticsearch-service:9200"
  elastic:
    enabled: true
    replicas: 1
    extraVolumes:
    # - name: backup-dir
    # mountPath: /mnt/nfs/backup/repo
    extraVolumeMounts:
    # - name: backup-dir
    # emptyDir: {}
    extraInitContainers:
    # - name: data-permissions
    # image: busybox
    # imagePullPolicy: IfNotPresent
    # command: ['sh', '-c', 'chown 1000 /data']
    # volumeMounts:
    # - name: elasticsearch-data
    #   mountPath: /data
    persistence: false
    memoryrequested: "1.3Gi"
    cpurequested: "500m"
    servicetype: NodePort
    memorylimit: "2Gi"
    cpulimit: "1000m"
    runAsUser:
    # storageClass: "sdstorageclass"
    ocp:
      syschroot: false
  kibana:
    image:
      registry: docker.elastic.co/
      name: kibana/kibana
      tag:
    enabled: true
    servicetype: NodePort
    memoryrequested: "400Mi"
    cpurequested: "300m"
    memorylimit: "400Mi"
    cpulimit: "1000m"
  fluentd:
    enabled: true
    elasticserver:
    elasticport: "9200"

service_efk:
  labels: {}
    #key1: value

fluentd:
  image:
    registry:
    name: bitnami/fluentd
    tag: 1.14.4-debian-10-r32
  memoryrequested: "80Mi"
  cpurequested: "300m"
  memorylimit: "250Mi"
  cpulimit: "500m"

envoy:
  image:
    registry:
    name: bitnami/envoy
    tag: 1.16.5-debian-10-r86

ingress:
  enabled: false
  annotations:
  host:
